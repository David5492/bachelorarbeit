{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\test\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:23:53.846858\n"
     ]
    }
   ],
   "source": [
    "# intendet steps:\n",
    "    # 1. Impute Missing Values\n",
    "    # 2. Transform vía scipy.stats.boxcox(data, lmbda=) (spare ich mir)\n",
    "    # 3. Outliers vía scipy.stats.mstats.winsorize(limits=[0.05,0.05])\n",
    "    # 4. Scaling vía StandardScaler() und MinMaxScaler()\n",
    "    # 5. NLP\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy import stats\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import random\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats.mstats import winsorize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Load data\n",
    "df= pd.read_csv(\"C:/Users/test/Documents/GitHub/bachelorarbeit/_climate_change_mitigation/data/interim/berlin_clean.csv\")\n",
    "\n",
    "# define subset lists\n",
    "text = ['description_misc','description_clear','equipment_clear','description_location', 'title']\n",
    "\n",
    "categorials = ['type','pets','condition','quality_of_appliances',\n",
    "                'heating_type','energy_certificate_type','ground_plan',\n",
    "                'energy_sources','parking_kind','hot_water_included',\n",
    "                'city_code','energy', 'energy_certificate']\n",
    "\n",
    "numericals = ['energy_consumption','rent','utilities_cost','heating_cost','cost_total',\n",
    "                'area','rooms','bedrooms','bathrooms','year_built',\n",
    "                'last_renovated','latitude','longitude','floor_act',\n",
    "                'floor_max','parking_spaces']\n",
    "\n",
    "\n",
    "# create sub dfs:\n",
    "df_categorials_dummies = pd.get_dummies(df[categorials])\n",
    "df_num = df[numericals]\n",
    "df_text = df[text] #.fillna('XXX') # nur testweise\n",
    "\n",
    "\n",
    "# Impute missing num \n",
    "\n",
    "median_impute = ['utilities_cost','heating_cost','latitude','longitude','floor_act',\n",
    "                'floor_max','parking_spaces']\n",
    "most_frequent_impute = ['bedrooms','bathrooms','year_built','last_renovated',]\n",
    "\n",
    "for col in median_impute:\n",
    "    imp_median = SimpleImputer(strategy='median')\n",
    "    df_num[col] = imp_median.fit_transform(df_num[[col]])\n",
    "\n",
    "for col in most_frequent_impute:\n",
    "    imp_mf = SimpleImputer(strategy='most_frequent')\n",
    "    df_num[col] = imp_mf.fit_transform(df_num[[col]])\n",
    "\n",
    "\n",
    "# Impute missing text \n",
    "\n",
    "def text_imputer(text):\n",
    "    \"\"\"\n",
    "    Rechnet mit einem str.\n",
    "    Prüft ob ein Wert nan ist und ersetzt ihn gegebenfalls mit Kauderwelsch.\n",
    "    Sinn: NLP funktioniert sonst nicht und bei einem anderen Impute entsteht\n",
    "    ein Bias. \n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=random.randint(50,100)))\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "for col in df_text.columns: \n",
    "    df_text[col] = df_text[col].apply(text_imputer)\n",
    "\n",
    "\n",
    "# Kill outliers by setting upper and lower limits\n",
    "\n",
    "for col in df_num.columns: \n",
    "    masked_array = winsorize(df_num[col], limits =[0.005,0.005])\n",
    "    df_num[col] = pd.DataFrame(masked_array, columns = [col])\n",
    "\n",
    "# Save data for EDA:\n",
    "df_EDA = pd.concat([df_num, df[categorials], df_text], axis=1)\n",
    "df_EDA = df_EDA.reset_index(drop=True)\n",
    "df_EDA.to_csv(\"C:/Users/test/Documents/GitHub/bachelorarbeit/_climate_change_mitigation/data/processed/berlin_preprocessed_EDA.csv\", index = False)\n",
    "\n",
    "\n",
    "# NLP:\n",
    "\n",
    "def text_process(annonce):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Produces word stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in annonce if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # remove any german stopwords\n",
    "    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('german')]\n",
    "\n",
    "    # Reduce words to their stem\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in nostop]\n",
    "\n",
    "# All cells of text in one col:\n",
    "df_text['all_cols'] = df_text.iloc[:,1:].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "# strings to token integer counts\n",
    "bow_transformer = CountVectorizer(analyzer=text_process, max_df=0.99, min_df=0.01).fit(df_text['all_cols']) \n",
    "\n",
    "#transform all annoces:\n",
    "annoces_bow = bow_transformer.transform(df_text['all_cols'])\n",
    "\n",
    "# tfidf-transformer:\n",
    "tfidf_transformer = TfidfTransformer().fit(annoces_bow)\n",
    "\n",
    "# transform the bow:\n",
    "annonces_tfidf = tfidf_transformer.transform(annoces_bow)\n",
    "\n",
    "# transform sparse matrix to pd.DataFrame\n",
    "df_former_sparse = pd.DataFrame.sparse.from_spmatrix(annonces_tfidf)\n",
    "\n",
    "stop = datetime.now()\n",
    "print(str(stop - start)) #just4fun\n",
    "\n",
    "# 0:23:53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Build RFRegressor for sorting the Importances of the words. \n",
    "y = df_num['energy_consumption']\n",
    "X = df_former_sparse\n",
    "     \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_train = MinMaxScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "X_test = MinMaxScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100, n_jobs=-1,\n",
       "                                             oob_score=False, random_state=42,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 10, 20],\n",
       "                         'max_features': ['auto', 'log2', 'sqrt'],\n",
       "                         'min_samples_leaf': [0.05, 0.1, 0.15],\n",
       "                         'n_estimators': [200, 300, 400]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "params_rf = {'n_estimators':[200,300,400],\n",
    "            'max_depth': [5,10,20],\n",
    "            'min_samples_leaf': [0.05, 0.1, 0.15],\n",
    "            'max_features': ['auto','log2','sqrt']}\n",
    "\n",
    "grid_rf = GridSearchCV(estimator = rf,\n",
    "                      param_grid = params_rf,\n",
    "                      cv=5,\n",
    "                      scoring = 'neg_mean_squared_error',\n",
    "                      n_jobs = -1)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 0.05, 'n_estimators': 200}\n",
      "-1508.516563474946\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=0.05,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=200, n_jobs=-1, oob_score=False,\n",
      "                      random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_rf.best_params_)\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:25:43.228329\n"
     ]
    }
   ],
   "source": [
    "# Extract most important words and add it to df for modelling\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(grid_rf.best_estimator_.feature_importances_)\n",
    "df_sorted_features = X[tree_importance_sorted_idx[:251]]\n",
    "\n",
    "# concat all sub-dfs\n",
    "df_all = pd.concat([df_num, df_categorials_dummies, df_sorted_features], axis=1)\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "# save data for modelling:\n",
    "df_all.to_csv(\"C:/Users/test/Documents/GitHub/bachelorarbeit/_climate_change_mitigation/data/processed/berlin_preprocessed.csv\", index = False)\n",
    "\n",
    "\n",
    "stop = datetime.now()\n",
    "print(str(stop - start)) #just4fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16911, 334)\n"
     ]
    }
   ],
   "source": [
    "print(df_all.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
